# Enhancing GenAI Performance on Intel AI Laptops using Intel® OpenVINO™

## Overview
This project aims to optimize the performance of Generative AI (GenAI) applications and simple Large Language Model (LLM) inference on Intel AI Laptops. By leveraging Intel® OpenVINO™, we focus on fine-tuning LLM models to run efficiently on CPUs, ensuring faster inference times and improved overall performance without significant loss in accuracy.

## Objectives
1. **Run GenAI Applications on Intel AI Laptops**:
   - Utilize the advanced processing capabilities of Intel AI Laptops to execute GenAI applications effectively.
   - Ensure seamless integration and execution of AI workloads on Intel hardware.

2. **Simple LLM Inference on CPU**:
   - Implement simple and efficient LLM inference directly on CPUs to utilize their full potential.
   - Reduce the dependency on high-power GPUs for LLM tasks, making it accessible on more devices.

3. **Fine-Tuning LLM Models using Intel® OpenVINO™**:
   - Convert standard LLM models to int8 quantization using Intel® OpenVINO™.
   - Fine-tune these models to achieve faster inference times while maintaining accuracy.

4. **Performance Comparison and Visualization**:
   - Test and evaluate the performance of both standard and fine-tuned models.
   - Visualize the differences in performance metrics such as inference time and accuracy.

## Methodology
1. **Model Configuration**:
   - Define the LLM model to be used and set up necessary parameters for processing.

2. **Quantization and Fine-Tuning**:
   - Utilize Intel® OpenVINO™ to convert and fine-tune the LLM models.
   - Apply int8 quantization to optimize model performance on CPUs.

3. **Inference Testing**:
   - Run inference tests on both standard and fine-tuned models.
   - Measure and record performance metrics to evaluate improvements.

4. **Performance Comparison**:
   - Use visualization tools like Matplotlib to compare the performance of standard and optimized models.
   - Highlight the advantages of using Intel® OpenVINO™ for fine-tuning LLMs.

## Technologies Used
- **Python**: For scripting and automation of model configuration and testing processes.
- **Intel AI Laptops**: To run GenAI applications and perform inference tests.
- **Intel® OpenVINO™**: For model quantization and fine-tuning to enhance performance.
- **PyTorch**: For handling LLMs and implementing necessary modifications.
- **Matplotlib**: For visualizing performance comparisons between standard and fine-tuned models.

## Expected Outcomes
- Demonstrate significant improvements in the performance of GenAI applications on Intel AI Laptops.
- Achieve faster LLM inference times on CPUs with minimal accuracy loss through int8 quantization.
- Provide a practical solution for deploying efficient AI applications on Intel hardware, making advanced AI capabilities more accessible.

## Team Members and Contribution
- **Roen Vishal Rao**: Model configuration and quantization.
- **Ajmal Aksar**: Performance testing and comparison.
- **Winston James Daniel B**: Performance testing and comparison.
- **Gideon Suttle**: Visualization and documentation.
- **Melvin Babu T**: Visualization and documentation.
